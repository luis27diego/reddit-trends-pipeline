services:
  minio:
    image: minio/minio:latest
    container_name: minio_storage
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - ./minio_data:/data
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server --console-address :9001 /data

  # -------------------------------------------------------
  # ðŸš€ CAMBIO CLAVE: Usamos el Dockerfile.worker para Spark
  # para garantizar Python 3.11 y los JARs de AWS
  # -------------------------------------------------------
  spark-master:
    build:
      context: .
      dockerfile: Dockerfile.worker
    container_name: spark-master
    hostname: spark-master
    ports:
      - "18080:8080"
      - "7077:7077"
    environment:
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
    # Sobrescribimos el CMD del Dockerfile para arrancar el Master
    command: >
      bash -c "
      /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
      --host spark-master
      --port 7077
      --webui-port 8080
      "

  spark-worker-1:
    build:
      context: .
      dockerfile: Dockerfile.worker
    container_name: spark-worker-1
    hostname: spark-worker-1
    depends_on:
      - spark-master
    environment:
      - SPARK_WORKER_CORES=4
      - SPARK_WORKER_MEMORY=4g
      # Apuntamos al Master
      - SPARK_MASTER_URL=spark://spark-master:7077
    ports:
      - "18081:8081"
    # Sobrescribimos el CMD del Dockerfile para arrancar el Worker
    command: >
      bash -c "
      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker
      spark://spark-master:7077
      "

  spark-worker-2:
    build:
      context: .
      dockerfile: Dockerfile.worker
    container_name: spark-worker-2
    hostname: spark-worker-2
    depends_on:
      - spark-master
    environment:
      - SPARK_WORKER_CORES=4
      - SPARK_WORKER_MEMORY=4g
      - SPARK_MASTER_URL=spark://spark-master:7077
    ports:
      - "18082:8081"
    command: >
      bash -c "
      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker
      spark://spark-master:7077
      "

  prefect-server:
    image: prefecthq/prefect:3-latest 
    container_name: prefect_server
    hostname: prefect-server
    ports:
      - "4200:4200"
    volumes:
      - ./prefect_data:/root/.prefect
    environment:
      PREFECT_SERVER_API_HOST: "0.0.0.0"
      PREFECT_API_URL: "http://prefect-server:4200/api" 
      PREFECT_UI_API_URL: "http://127.0.0.1:4200/api" 
    command: prefect server start --host 0.0.0.0
    restart: always

  prefect-worker:
    build:
      context: .
      dockerfile: Dockerfile.worker 
    container_name: prefect_process_worker
    volumes:
      - ./flows:/app/flows 
    environment:
      SPARK_HOME: /opt/spark
      PREFECT_API_URL: http://prefect-server:4200/api
      # Importante: Asegurar que el driver sepa su propia IP o hostname
      SPARK_DRIVER_HOST: prefect-worker 
    depends_on:
      - prefect-server
      - spark-master
    restart: always
    command: >
      bash -c "
      sleep 10 &&
      prefect worker start --pool default
      "

networks:
  default:
    name: spark-minio-net