services:
  minio:
    image: minio/minio:latest
    container_name: minio_storage
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - ./minio_data:/data
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    command: server --console-address :9001 /data

  # -------------------------------------------------------
  # üöÄ CAMBIO CLAVE: Usamos el docker/worker/Dockerfile para Spark
  # para garantizar Python 3.11 y los JARs de AWS
  # -------------------------------------------------------
  spark-master:
    build:
      context: .
      dockerfile: docker/worker/Dockerfile
    container_name: spark-master
    hostname: spark-master
    ports:
      - "18080:8080"
      - "7077:7077"
    environment:
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
    # Sobrescribimos el CMD del Dockerfile para arrancar el Master
    command: >
      bash -c "
      /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
      --host spark-master
      --port 7077
      --webui-port 8080
      "

  spark-worker-1:
    build:
      context: .
      dockerfile: docker/worker/Dockerfile
    container_name: spark-worker-1
    hostname: spark-worker-1
    depends_on:
      - spark-master
    environment:
      - SPARK_WORKER_CORES=${SPARK_WORKER_CORES}
      - SPARK_WORKER_MEMORY=${SPARK_WORKER_MEMORY}
      # Apuntamos al Master
      - SPARK_MASTER_URL=${SPARK_MASTER_URL}
    ports:
      - "18081:8081"
    # Sobrescribimos el CMD del Dockerfile para arrancar el Worker
    command: >
      bash -c "
      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker
      ${SPARK_MASTER_URL}
      "

  spark-worker-2:
    build:
      context: .
      dockerfile: docker/worker/Dockerfile
    container_name: spark-worker-2
    hostname: spark-worker-2
    depends_on:
      - spark-master
    environment:
      - SPARK_WORKER_CORES=${SPARK_WORKER_CORES}
      - SPARK_WORKER_MEMORY=${SPARK_WORKER_MEMORY}
      - SPARK_MASTER_URL=${SPARK_MASTER_URL}
    ports:
      - "18082:8081"
    command: >
      bash -c "
      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker
      ${SPARK_MASTER_URL}
      "

  postgres:
    image: postgres:15
    container_name: prefect_postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./sql/init:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]
      interval: 5s
      timeout: 5s
      retries: 5
    ports:
      - "5432:5432"

  prefect-server:
    image: prefecthq/prefect:3-latest 
    container_name: prefect_server
    hostname: prefect-server
    ports:
      - "4200:4200"
    volumes:
      - ./prefect_data:/root/.prefect
    environment:
      PREFECT_SERVER_API_HOST: "0.0.0.0"
      PREFECT_API_URL: ${PREFECT_API_URL}
      PREFECT_UI_API_URL: ${PREFECT_UI_API_URL}
      # Conexi√≥n a PostgreSQL
      PREFECT_API_DATABASE_CONNECTION_URL: "postgresql+asyncpg://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}"
    depends_on:
      postgres:
        condition: service_healthy
    command: prefect server start --host 0.0.0.0
    restart: always

  prefect-worker:
    build:
      context: .
      dockerfile: docker/worker/Dockerfile 
    container_name: prefect_process_worker
    volumes:
      - ./flows:/app/flows 
    environment:
      SPARK_HOME: /opt/spark
      PREFECT_API_URL: ${PREFECT_API_URL}
      SPARK_DRIVER_HOST: prefect-worker
      SPARK_MASTER_URL: ${SPARK_MASTER_URL}
      # MinIO credentials for Spark sessions
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      MINIO_ENDPOINT: ${MINIO_ENDPOINT}
      MINIO_BUCKET: ${MINIO_BUCKET}
      # Database credentials
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_HOST: ${POSTGRES_HOST}
    depends_on:
      - prefect-server
      - spark-master
    restart: always
    command: >
      bash -c "
      sleep 10 &&
      prefect worker start --pool default
      "
  # 1. Tu API de Python (FastAPI)
  api:
    build: 
      context: .
      dockerfile: docker/api/Dockerfile
    container_name: reddit_api
    ports:
      - "8000:8000"
    environment:
      # Conexi√≥n a  la base 'reddit_analytics'
      DATABASE_URL: "postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/reddit_analytics"
    depends_on:
      postgres:
        condition: service_healthy
    volumes:
      - ./src:/app/src  # Hot-reload para desarrollo

  # 2. Metabase (Tu BI Visual)
  metabase:
    image: metabase/metabase:latest
    container_name: reddit_bi
    ports:
      - "3000:3000"
    environment:
      MB_DB_TYPE: postgres
      MB_DB_DBNAME: metabase_appdb 
      MB_DB_PORT: 5432
      MB_DB_USER: ${POSTGRES_USER}
      MB_DB_PASS: ${POSTGRES_PASSWORD}
      MB_DB_HOST: postgres
    depends_on:
      postgres:
        condition: service_healthy
    volumes:
      - metabase_data:/metabase-data
volumes:
  postgres_data:
  metabase_data:

networks:
  default:
    name: spark-minio-net