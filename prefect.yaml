# Welcome to your prefect.yaml file! You can use this file for storing and managing
# configuration for deploying your flows. We recommend committing this file to source
# control along with your flow code.

# Generic metadata about this project
name: proyecto-tendencias
prefect-version: 3.6.4

# build section allows you to manage and build docker images
build: null

# push section allows you to manage if and how this project is uploaded to remote locations
push: null

# pull section allows you to provide instructions for cloning this project in remote locations
pull:
- prefect.deployments.steps.git_clone:
    repository: https://github.com/luis27diego/reddit-trends-pipeline
    branch: main

# the deployments section allows you to provide configuration for deploying flows
deployments:
- name: ingesta-reddit-deployment
  version: null
  tags: []
  concurrency_limit: null
  description: null
  # Asumiendo que el flow de ingesta está en la carpeta 'flows'
  entrypoint: flows/ingesta/flujo_ingesta.py:flujo_ingesta
  parameters: {}
  work_pool:
    name: default
    work_queue_name: null
    job_variables: {}
  schedules:
  - interval: 1200.0
    anchor_date: '2025-11-23T15:33:42.727803+00:00'
    timezone: UTC
    active: true

#  NUEVO DEPLOYMENT: Reemplaza la ejecución de Databricks por Spark Local Dockerizado
- name: spark-local-processing-deployment
  version: null
  # Etiquetas actualizadas
  tags: ["spark", "procesamiento", "docker"] 
  concurrency_limit: null
  description: "Procesa archivos usando PySpark conectado a un clúster Spark Standalone en Docker."
  #  IMPORTANTE: El 'entrypoint' ahora apunta al nuevo flow local
  entrypoint: flows/procesamiento_reddit.py:flujo_procesamiento_local
  parameters:
    # Este valor por defecto solo se usa si se ejecuta manualmente. 
    # Cuando lo llama el flow de ingesta, se sobrescribe.
    minio_key_entrada: "raw/1342-0.txt"
  work_pool:
    name: default
    work_queue_name: null
    job_variables: {}
  schedules: []